\documentclass{article}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{url}
\usepackage[linewidth=1pt]{mdframed}
\DeclareMathOperator{\mbps}{Mbits/s}
\begin{document}

\title{CS 244 - PA2 - Congestion-Control Contest}
\author{Praveen Kumar}

\maketitle


\textbf{Summary:}\\
Name: Latte\\
Code: \url{https://github.com/prvnkumar/latte-cc}\\
Performance:
\url{http://cs344g.keithw.org/report/?Latte-1493317091-ifaikaix}
\begin{mdframed}
\begin{alltt}
Average capacity: 5.04 Mbits/s
Average throughput: 3.83 Mbits/s (76.0\% utilization)
95th percentile per-packet queueing delay: 49 ms
95th percentile signal delay: 93 ms
\end{alltt}
\textbf{*** Power score = 41.18 ***}
\end{mdframed}
\clearpage
\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../A/A"}
\caption{Throughput vs. 95-percentile signal delay for different values of window size}
\label{A}
\end{figure}


\section{A - Vary window size}
\subsection{Experiment}
We vary the window size in controller.cc and record the throughput and
delay statistics. Fig.~\ref{A} shows the 2D graph of throughput vs.
95-percentile signal delay for different values of window size.
\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../A/A"}
\caption{Throughput vs. 95-percentile signal delay for different values of window size}
\label{A}
\end{figure}

\subsection{Best window size}
To find the best window size, we plot the value of score defined as:
\[
  score = \frac{throughput}{95~percentile\ signal\ delay}
\]
in Fig.~\ref{A-score}.
\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../A/A-score"}
\caption{Score vs window size}
\label{A-score}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../A/A-score-log"}
\caption{$\log(Score)$ vs window size}
\label{A-score-log}
\end{figure}

Fig.~\ref{A-score-log} shows essentially the same
data\footnote{Problem description said to use $\log$}. From these
graphs, we can infer that the optimum static window size is 12.

\subsection{Repeatability}
We repeated the experiments with window size = 12 five times.\\
Throughput: $\mu = 2.282 \mbps, \sigma = 0.004 \mbps$ \\
95th percentile signal delay: $\mu = 177.2 ms, \sigma = 0.748 ms$\\
So, the experiments are highly repeatable.

\clearpage
\section{B - AIMD scheme}
\subsection{Implementation}
We implemented a simple additive increase multiplicative decrease
mechanism to dynamically adjust the window size as follows:
\begin{alltt}
  On ACK receipt:
    /* Additive increase */
    cwnd_ += aimd_inc_param_/cwnd_;

  On timeout:
    /* Multiplicative decrease */
    if (cwnd_ > 1) \{
      cwnd_ /= aimd_dec_param_;
    \}
\end{alltt}

where \texttt{aimd\_inc\_param\_} and \texttt{aimd\_dec\_param\_} are the
AIMD parameters.

\subsection{Parameters}
We tried two different values of starting window size -- default (50)
and the optimal static value of (12).\\
In terms of additive increase, we tried the usual value of 1 per RTT,
along with more aggressive values of 5 and 10 as explained later. We
kept the multiplicative decrease factor to 2 because it is good enough
to backoff rapidly on congestion. We also tried different values of
timeout parameter -- default (1000) and a more aggressive (100).

\subsection{Evaluation}
Overall, we observe that AIMD doesn't perform well as explained next.
Since, the window size gets changed dynamically, the starting value
of window size doesn't affect the behavior much. Fig.~\ref{aimd}
shows throughput vs delay for different values of the parameters.
Similarly, Fig.~\ref{aimd-score} shows score for different
parameter values.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../B/aimd"}
\caption{Throughput vs. 95-percentile signal delay for different
values of parameters when using AIMD. Labels are: (initial window
size, additive increases param, multiplicative decrease param, timeout
interval)}
\label{aimd}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../B/aimd-score"}
\caption{Score vs. Initial window for different
values of parameters when using AIMD. Labels are: (initial window
size, additive increases param, multiplicative decrease param, timeout
interval)}
\label{aimd-score}
\end{figure}

An additive increase of 1 per RTT results in low throughput as we are
not able to fill the pipe quickly when bandwidth is available. So, we
increase this parameter to increase throughput, but this also causes
more queueing delay and thus decreases score.

\clearpage
\section{C - delay-triggered scheme}
We tried two different schemes -- simple and AIMD based on RTT.
\subsection{Simple}
The simple approach is additive increase/decrease based on whether the
latest measured RTT is less than or greater than a threshold
(rtt\_thresh).
\begin{alltt}
  if (rtt < rtt_thresh_):
    cwnd_ += 1/cwnd_;

  if (rtt >= rtt_thresh_):
      cwnd_ -= 1/cwnd_;
\end{alltt}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../C/simple/rtt"}
\caption{Throughput vs. 95-percentile signal delay for different
values of rtt\_thresh when using simple delay based approach. Labels
show rtt\_thresh}
\label{rtt}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../C/simple/rtt-score"}
\caption{Score vs. rtt\_thresh when using simple delay based approach}
\label{rtt-score}
\end{figure}
From Fig.~\ref{rtt}, we can see that increasing $rtt\_thresh$
increases throughput but also increases latency. As seen in
Fig.~\ref{rtt-score}, the maximum score is achieved at $rtt\_thresh =
75ms$. Another issue with this approach is that the value of window
can become less than 1, leading to timeouts before packet transmission
resumes. This can be see for extremely low values of RTT threshold. We
handle this in the next approach.

\clearpage
\subsection{RTT + AIMD}
In this approach, we modify the window in an AIMD manner based on
whether the latest RTT is less than or greater than a threshold
(rtt\_thresh).
\begin{alltt}
if (rtt < rtt_thresh_):
    cwnd_ += 1/cwnd_;

else:
    if (cwnd_ >= 2):
      cwnd_ = cwnd_/2;
\end{alltt}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../C/aimd/aimdrtt"}
\caption{Throughput vs. 95-percentile signal delay for different
values of rtt\_thresh when using RTT+AIMD based approach. Labels
show rtt\_thresh}
\label{aimdrtt}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{"../C/aimd/aimdrtt-score"}
\caption{Score vs. rtt\_thresh when using RTT+AIMD based approach}
\label{aimdrtt-score}
\end{figure}

This improves on the previous method of simply increasing or
decreasing the congestion window linearly based on whether the RTT was
below or above a threshold. The performance becomes more predictable
and follows a tradeoff curve. From Fig.~\ref{aimdrtt}, we see that
increasing $rtt\_thresh$ increases throughput but also increases
latency.\\

As seen in Fig.~\ref{aimdrtt-score}, the maximum score of 22 is
achieved at $rtt\_thresh = 120ms$.

\clearpage
\section{D - The Contest}

The network trace provided is highly unstable. So, our system tries to
estimate the steady state whenever the network is stable, and reacts
quickly on detecting congestion.\\

\subsection{Latte - Design Approaches Considered}
\subsubsection{Steady state}
For the steady state behaviour, we measure the RTT and bottleneck
bandwidth in manner similar to BBR~\cite{bbr}. Let $rtt_t$ be an RTT
sample received at time $t$. We compute $min\_rtt$ as the minimum
value of $rtt_t$ as observed in time window of $W_R$. Similarly,
whenever an ACK arrives, we update the amount of data that has been
delivered. We keep a sliding window of amount of the data delivered.
From this, we can estimate the delivery rate. Unlike BBR, we don't
use the ACK to bring this information because the current packet
format in the framework doesn't have such a field. The bottleneck
bandwidth, similarly, is the maximum delivery rate in a window
$W_B$.\\

Updating congestion window: From the estimated RTT and bandwidth,
we can compute the BDP. We set the congestion window = $\lambda \times
BDP$, where $\lambda$ is a configurable parameter.\\
\[
cwnd = \lambda \times max\_bw \times min\_rtt
\]
Sending packets: Whenever the congestion window allows packets to be
sent, we schedule packets to be transmitted at regular intervals =
$\gamma$ / $bandwidth$, where $\gamma$ is configurable.  In BBR,
$\gamma$ is also modulated periodically by \textit{cycle gain} to
detect any increase in bandwidth.\\
$\lambda$ and $\gamma$ are similar to $cwd\_gain$ and $pacing\_gain$
in BBR.\\
This works pretty well if the network is in a steady state. But, the
trace shows that the network capacity keeps changing rapidly.


\subsubsection{Fast reaction to congestion}
\paragraph{Observation} The bottleneck bandwidth for cellular networks
is volatile, and BBR's approach leads to high latency when this
bandwidth decreases. Consider Figure~\ref{bbrdelay} from the the
original paper. When the bandwidth has decreased, it causes increased
RTT as the sender is still sending at a higher rate. Because RTT
estimation happens before this bottleneck is detected, this increases
the number of inflight packets and further increases queueing delay.
RTT starts decreasing only after the maximum bottleneck window that
caused high BDP goes out of bandwidth window, which can be of the
order of a few RTTs. So, for this entire duration, the network
experiences high delay.

\begin{figure}[!h]
\includegraphics[width=0.8\columnwidth]{"bbrdelay"}
\caption{BBR's response to change in bandwidth. (Source: BBR paper~\cite{bbr} Fig 3)}
\label{bbrdelay}
\end{figure}

\paragraph{Solution} To react to such increase in RTT, we want to
quickly cut down the number of packets in flight that may cause
queueing delays. So, whenever the sender sees a RTT higher than
$min\_rtt$, one approach that cuts down the congestion window by a
factor exponential to the relative increase in RTT could be:
\[
  cwnd = \frac{cwnd}{e^{(\frac{rtt_t}{min\_rtt} - 1})}
\]

This approach is robust to noise in RTT measurements because even if
the sender receives a bad sample with high RTT, the congestion window
changes for a very small interval. It will be updated when the next
ACK is received. Also, this exponential penalty doesn't affect the
congestion window much when measured RTT is close to $min\_rtt$. There
are other functions that can be used instead of the exponential
function. But, the key takeaway is to \emph{avoid build-up of queueing delay
on detecting high RTT when bottleneck bandwidth decreases}.

\subsubsection{RTT gradient approach (TIMELY)}
Another way to detect build up of congestion could be to look at the
RTT gradient instead of just  RTT values~\cite{timely}. This
approach could detect that that RTT is increasing and react to it
before RTT reaches a high value. We estimated the gradient as:

\[
  rtt\_grad_t = \frac{rtt_t - last\_rtt_t}{min\_rtt} \\
\]
\[
  rtt\_grad = (1 - \alpha) * rtt\_grad + \alpha * rtt\_grad_t;
\]

where $\alpha$ is EWMA smoothing factor.
On detecting that that RTT is increasing, or equivalently, RTT
gradient is positive, the sender reacts by cutting down the congestion
window multiplicatively.

\[
  cwnd = cwnd \times ( 1 - \beta \times rtt\_grad)
\]

We found that this approach didn't result in any significant
improvement for the given trace.

\subsubsection{BBR's cycle gain}
BBR uses cycle gain to periodically increase and decrease the pacing
rates to probe for increase in bandwidth. We experimented with various
gain factors and concluded that the net benefit of cycle gain was
overshadowed by setting an optimal value of $\lambda$.

\paragraph{Timeout} We found that setting a good value of timeout is
critical to reducing the signal delay. We find empirically that a
value of $1.5 \times min\_rtt$ works well. Once a timeout happens, we
transmit one packet to act as a probe and reset the timer.

\subsection{Latte: Final design}
Our final design consists of dynamic bottleneck bandwidth and minimum
RTT detection, augmented with aggressive backoff on high RTT
detection.\\

In order to keep the system simple, we removed any approaches (e.g.
cycle gain) that we tested and didn't result in significant
improvement. Latte considers the latest RTT sample and compares with
$min\_rtt$. In our system, since the congestion window is updated on
every ACK, based on measured bandwidth and RTT, we can quickly adjust
the congestion window on every ACK. Whenever we receive an ACK with
higher RTT, we decrease the congestion window by a factor of $rtt_t /
min\_rtt$.  Thus, the congestion window effectively becomes:
\[
  \lambda \times bandwidth \times min\_rtt^2 / rtt_t
\]
when $rtt_t$ is high. This ensures that we do not create congestion in
the steady state and $rtt_t$ are close to $min\_rtt$. 

\clearpage
\subsubsection{Performance}
\paragraph{Overfit}
Based on experimenting with different values, we set $\lambda = 1.7$
and $\gamma = 0.8$.  This worked really well for the given trace, and
after optimizing the control parameters, we were able to achieve a
\textbf{power score of 42.56} on the given trace.

\url{http://cs344g.keithw.org/report/?Latte-1492997567-eezahmoy}\\

\begin{mdframed}
\begin{alltt}
Average capacity: 5.04 Mbits/s
Average throughput: 3.83 Mbits/s (75.9\% utilization)
95th percentile per-packet queueing delay: 50 ms
95th percentile signal delay: 90 ms
Power Score: 42.56
\end{alltt}
\end{mdframed}

\begin{figure}[h]
\includegraphics[width=1.2\columnwidth]{"tput"}
\caption{Throughput - Latte on verizon trace}
\label{tput}
\end{figure}

\begin{figure}[h]
\includegraphics[width=1.2\columnwidth]{"delay"}
\caption{Signal and packet delay - latte on verizon trace}
\label{delay}
\end{figure}


\paragraph{Generalization} However, the above parameters seem like
overfitting for the particular trace. So, we added a logic to set
$\lambda$ dynamically on the value of $cwnd$ that i) keeps
$\lambda$ high at low congestion window to prevent stalling, and ii)
decreases $\lambda$ as congestion window increases to decrease the
probability (and amount) of overshooting capacity. As a result, the
performance on the given trace decreases. The results are at:\\
\url{http://cs344g.keithw.org/report/?Latte-1493317091-ifaikaix}

\begin{mdframed}
\begin{alltt}
Average capacity: 5.04 Mbits/s
Average throughput: 3.83 Mbits/s (76.0\% utilization)
95th percentile per-packet queueing delay: 49 ms
95th percentile signal delay: 93 ms
\end{alltt}

\textbf{*** Power score = 41.18 ***}
\end{mdframed}

The corresponding throughput and delay time series are shown in
Figures~\ref{tput} and ~\ref{delay}.

\subsection{Conclusion} Our key observation was that the network
should perform close to Kleinrock's optimal point in network steady
state, but react aggresively to increase in RTT to achieve high power
score. We build Latte along this lines by following bottleneck
bandwidth and RTT estimation similar to BBR and augment it with
multiplicative decrease based on observed RTT.

\bibliographystyle{unsrt}
\bibliography{report}
\end{document}
